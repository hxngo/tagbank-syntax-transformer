# 모델 설정
model:
  vocab_size: 30522  # BERT의 기본 vocab size로 변경
  d_model: 768
  n_heads: 12
  n_encoder_layers: 6
  d_ff: 3072
  dropout: 0.2  # 드롭아웃 증가
  attention_dropout: 0.1  # 어텐션 드롭아웃 추가
  max_length: 512
  num_classes: 3  # 다시 3으로 변경 (O, NP, VP 태그)
  special_tokens:
    pad_token_id: 0
    mask_token_id: 103  # BERT의 [MASK] 토큰 ID
    cls_token_id: 101   # BERT의 [CLS] 토큰 ID
    sep_token_id: 102   # BERT의 [SEP] 토큰 ID

# 훈련 설정
training:
  batch_size: 64  # 배치 크기 감소
  epochs: 100
  learning_rate: 0.0003  # 학습률 증가
  weight_decay: 0.1  # 가중치 감쇠 증가
  warmup_ratio: 0.1  # 웜업 비율 증가
  grad_clip: 0.5  # 그래디언트 클리핑 감소
  label_smoothing: 0.1  # 레이블 스무딩 증가
  early_stopping:
    patience: 5  # 조기 종료 인내심 감소
    min_delta: 0.001
  min_epochs: 10  # 최소 에폭 감소

# 데이터 설정
data:
  train_data: "../data/large_tagbank/train_augmented.tag"
  val_data: "../data/large_tagbank/validation.tag"
  test_data: "../data/large_tagbank/test.tag"
  max_seq_len: 512
  val_ratio: 0.2
  class_weights: null  # 자동 계산 또는 직접 지정
  sampling_strategy: "balanced"  # 균형잡힌 샘플링
  
  # 데이터 로딩 설정
  num_workers: 8  # 데이터 로딩 워커 수
  pin_memory: true  # GPU 전송 최적화
  prefetch_factor: 4  # 미리 로딩할 배치 수
  persistent_workers: true  # 워커 재사용
  
  # 데이터 증강 설정
  augmentation:
    enabled: true
    methods:
      - "random_mask"
      - "token_substitute"
    prob: 0.3  # 전체 증강 확률
    aug_prob: 0.7  # 증강을 적용할 확률
    mask_prob: 0.15  # 토큰을 마스킹할 확률
    shuffle_prob: 0.1  # 토큰 순서를 섞을 확률
    max_shuffle_distance: 3  # 토큰 셔플 시 최대 이동 거리
    token_range:
      min_id: 1000
      max_id: 29000
    class_range:
      min_id: 0
      max_id: 2  # 3개 클래스 (0, 1, 2)
    mask_token_id: 103  # [MASK] 토큰 ID
    pad_token_id: 0  # [PAD] 토큰 ID
    cls_token_id: 101  # [CLS] 토큰 ID
    sep_token_id: 102  # [SEP] 토큰 ID

# 최적화 설정
optimization:
  gradient_checkpointing: true
  gradient_accumulation_steps: 1  # 배치 크기가 커졌으므로 누적 스텝 감소
  mixed_precision: true
  memory_efficient_attention: true
  compile_model: true  # torch.compile 사용

# 평가 설정
evaluation:
  eval_steps: 100
  metrics:
    - "accuracy"
    - "f1"
    - "syntax_accuracy"  # 구문 구조 예측 정확도
  best_metric: "f1"  # F1 스코어 기준으로 최고 모델 선택
  save_top_k: 3

# 손실 함수 설정
loss:
  main_loss: "cross_entropy"
  auxiliary_losses:
    syntax_loss:  # 구문 구조 예측을 위한 추가 손실
      enabled: true
      weight: 0.3
    consistency_loss:  # 구문 일관성을 위한 손실
      enabled: true
      weight: 0.2

# 로깅 설정
logging:
  project_name: "tag-transformer"
  log_interval: 10
  save_dir: "../checkpoints" 